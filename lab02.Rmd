---
title: "Lab 02"
author: "[your name here]"
date: "`r Sys.Date()`"
---

# Brief description

In this lab, we will perform ridge regression on the prostate cancer
data set studied last time. Recall that the 9th variable, `lpsa`, is the
response while the rest are potential predictors. the
variable `lpsa` in the 9th column is the response while the rest are
potential predictors. The data set is available in the `{ElemStatLearn}` 
package and a description can be found at
<http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.info.txt>


# Questions

Like last time, read the data into R using the following command (with
correct file path and name):

```{r, message=FALSE, warning=FALSE}
data(prostate, package = "ElemStatLearn")
library(tidyverse)
prostate <- prostate |> select(-train)
```


## Q1

What are the coefficients for the predictors `lcavol`,
`svi` and `lcp` in the full linear regression model? (If you recall,
these are the three predictors most correlated with the response.)

::: {.solbox data-latex=""}
```{r}

# some code

```
:::


For responses $y_{1},\ldots,y_{n}$, vector of predictors
$\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{n}$ and fixed penalization
parameter $\lambda$, ridge regression solves the optimization
problem
$$
\hat{\boldsymbol{\beta}} = 
\underset{\boldsymbol{\beta}}{\arg\min}
\left[\sum_{i=1}^{n}(y_{i}-\boldsymbol{x}_{i}^{\intercal}\boldsymbol{\beta})^{2}
+ \lambda\boldsymbol{\beta}^{\intercal}\boldsymbol{\beta}\right],
\label{eq:ridgereg}
$$
where $\boldsymbol{\beta}$ is the parameter vector (regression
coefficients) and $\lambda$ is a tuning parameter that penalizes
$\boldsymbol{\beta}$'s that are "too large". It can be shown that the
solution to ridge regression is
$$
\hat{\boldsymbol{\beta}} = (\boldsymbol{X}^{\intercal}\boldsymbol{X} +
\lambda\boldsymbol{I})^{-1}\boldsymbol{X}^{\intercal}\boldsymbol{y},
\label{eq:estpar}
$$
where $\boldsymbol{X}$ is the design matrix, $\boldsymbol{y}$ is the
response vector and $\boldsymbol{I}$ is the identity matrix of
appropriate size.

## Q2

What is the value of $\lambda$ for the regression
considered in **Q1**? As $\lambda\to\infty$, what is the behaviour
of the vector $\hat{\boldsymbol{\beta}}$ (the ridge regression estimator)?

::: {.solbox data-latex=""}

(some text)

:::

Now, we perform ridge regression using the function `glmnet` in the
package of the same name.Load the package using
`library(glmnet)`. Read the documentation of the function by typing
`?glmnet` in the console. We are interested in these four arguments of
the function: `x`, `y`, `alpha` and `lambda`.

We consider the sequence of penalties
$\boldsymbol{\lambda}=(e^{-3},e^{-2.8},e^{-2.6},\ldots,e^{2.6},e^{2.8},e^{3})$.
Create this vector of length 31 and store it as variable `lam`. Fit the
series of ridge regressions to the data set. You will need to use the
following command:

```{r, eval = FALSE}
fittedobject <- glmnet(x = ___, y = ___, alpha = 0, lambda = ___)
```

where `alpha = 0` specifies the ridge regression (as opposed to other
shrinkage methods). Replace the `___` by the appropriate variable names.

**Hint**: The argument `x` accepts a matrix. You can use `as.matrix()` to
convert a data frame to matrix. Or, combine a formula with `model.matrix()`
like `model.matrix(y ~ var1 + var2, data = df)`. Try `?model.matrix`. Note that
`model.matrix()` adds an intercept column, which `glmnet()` does as well, so
you'll have to get rid of one if you go this route.

::: {.solbox data-latex=""}

```{r q2-solution, message=FALSE, warning=FALSE}
library(glmnet)

# some code

```
:::

## Q3

Notice that the fitted `glmnet` object is a list with many components. Inspect
it by looking in the Environmet tab, printing it, calling `names(obj)` and / or
trying `str(obj)`. Like most modelling routines in `R`, there is a `coef()` 
method for `glmnet` objects.

Write down the coefficients for the predictors `lcavol`, `svi` and
`lcp` when $\lambda=e^{3}$ and $\lambda=e^{0}$, respectively.

**Hint**: `fittedobject` is a list. Variables in a list can be
extracted using the `$` operator. For example, `fittedobject$lambda`
gives you the $\lambda$ values used in the series of ridge
regressions.

::: {.solbox data-latex=""}
```{r q3-solution}
# some code
```
:::

Next, we find the optimal value of $\lambda$ (that yields the best
predictive ability) via cross validation. The relevant function is
`cv.glmnet()`. We will also need to use the `nfolds` argument in addition
to those above (check the documentation of this function).

## Q4

Carry out a 5-fold cross validation, using the same
$\lambda$ values as above. Run `set.seed(406)` **immediately
before** the `cv.glmnet()` function to obtain reproducible
results. Inspect the fitted object; which variable stores the cross
validation mean squared errors?

::: {.solbox data-latex=""}
```{r q4-solution}
set.seed(406)

# some code

```

(some text)

:::

## Q5

What is the value of $\lambda$ that minimizes the CV error and what is its
associated mean squared error?

**Hint**: You can visualize the result using `plot()` on the fitted
object in **Q4**.

::: {.solbox data-latex=""}

```{r q5-solution}

# some code

```
:::

## Q6

For this (optimal) $\lambda$, what are the fitted
coefficients for the predictors `lcavol`, `svi` and `lcp`?

::: {.solbox data-latex=""}
```{r q6-sol}
# some code
```
:::